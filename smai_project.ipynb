{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smai_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "4vn-kIRHorTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "TEXT_SIZE = 16\n",
        "KEY_SIZE = 16"
      ],
      "metadata": {
        "id": "JnQ0jKmT38Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AllyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.connected = nn.Linear(TEXT_SIZE + KEY_SIZE, TEXT_SIZE + KEY_SIZE)\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 2, kernel_size = 4, stride=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 2, out_channels = 4, kernel_size = 2, stride=2)\n",
        "        self.conv3 = nn.Conv1d(in_channels = 4, out_channels = 4, kernel_size = 1, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels = 4, out_channels = 1, kernel_size = 1, stride=1)\n",
        "        \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        # print(0)\n",
        "        x = x.unsqueeze(0)\n",
        "        x = self.connected(x)\n",
        "        # print(1)\n",
        "        x = F.pad(x, (1,2))\n",
        "        # print(2)\n",
        "        x = self.sigmoid(x)\n",
        "        # print(3)\n",
        "\n",
        "        x = x.unsqueeze(0)\n",
        "        x = self.conv1(x)\n",
        "        # print(4)\n",
        "        x = F.pad(x, (0,1))\n",
        "        # print(5)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.tanh(x)\n",
        "        x = x.squeeze(0)\n",
        "        x = x.squeeze(0)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rpk03PeK3yX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdversaryNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.connected = nn.Linear(TEXT_SIZE, TEXT_SIZE)\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 2, kernel_size = 4, stride=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 2, out_channels = 4, kernel_size = 2, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels = 4, out_channels = 4, kernel_size = 1, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels = 4, out_channels = 1, kernel_size = 1, stride=1)\n",
        "        \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = x.unsqueeze(0)\n",
        "        x = self.connected(x)\n",
        "        # print(1)\n",
        "        x = F.pad(x, (1,2))\n",
        "        # print(2)\n",
        "        x = self.sigmoid(x)\n",
        "        # print(3)\n",
        "\n",
        "        x = x.unsqueeze(0)\n",
        "        x = self.conv1(x)\n",
        "        # print(4)\n",
        "        x = F.pad(x, (0,1))\n",
        "        # print(5)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.tanh(x)\n",
        "        x = x.squeeze(0)\n",
        "        x = x.squeeze(0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CpfT9AgBGSpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eves_loss = (1/batch_size)*tf.reduce_sum( tf.abs( Eve_out_message - Alice_input_message ))\n",
        "def eveLoss(aliceInput,eveOutput):\n",
        "  aliceInput = (aliceInput + 1)/2\n",
        "  eveOutput = (eveOutput + 1)/2\n",
        "  loss = nn.L1Loss()\n",
        "  loss = loss(aliceInput,eveOutput)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def aliceBobLoss(aliceInput, bobOutput, eveLoss):\n",
        "  aliceInput = (aliceInput + 1)/2\n",
        "  bobOutput = (bobOutput + 1)/2\n",
        "  bobLoss = nn.L1Loss()\n",
        "  bobLoss = bobLoss(aliceInput,bobOutput)\n",
        "\n",
        "  eveEvadroppingLoss = (((TEXT_SIZE/2)-eveLoss)*((TEXT_SIZE/2)-eveLoss))/((TEXT_SIZE/2)*(TEXT_SIZE/2))\n",
        "\n",
        "  finalLoss =  bobLoss + eveEvadroppingLoss\n",
        "  return finalLoss\n",
        "\n"
      ],
      "metadata": {
        "id": "Lozl4G4WRkDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = torch.tensor([1,-1,1,-1,-1,-1,1], dtype=torch.float32)\n",
        "# target = torch.tensor([1,-1,1,-1,-1,-1,-1], dtype=torch.float32)\n",
        "# i = eveLoss(input,target)\n",
        "# print(i,type(i))\n",
        "# i = aliceBobLoss(input, target, i)\n",
        "# print(i,type(i))"
      ],
      "metadata": {
        "id": "nHBRXH_KZ_VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Alice = AllyNetwork().cuda()\n",
        "Bob = AllyNetwork().cuda()\n",
        "Eve = AdversaryNetwork().cuda()\n",
        "\n",
        "optimizerAlice = optim.Adam(Alice.parameters())\n",
        "optimizerBob = optim.Adam(Bob.parameters())\n",
        "optimizerEve = optim.Adam(Eve.parameters())"
      ],
      "metadata": {
        "id": "Ru9PkLFKlgfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "g2aMrvP2CNsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def geterateDataset(msgLen,keyLen,size):\n",
        "  dsMsg = []\n",
        "  for i in range(size):\n",
        "    haha = torch.randint(0,2,(msgLen,))\n",
        "    haha = (haha*2)-1\n",
        "    haha = haha.float()\n",
        "    dsMsg.append(haha)\n",
        "\n",
        "  dsKey = []\n",
        "  for i in range(size):\n",
        "    haha = torch.randint(0,2,(keyLen,))\n",
        "    haha = (haha*2)-1\n",
        "    haha = haha.float()\n",
        "    dsKey.append(haha)\n",
        "  return  dsMsg,  dsKey\n",
        "\n"
      ],
      "metadata": {
        "id": "Qa3ixrpMndRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 4096\n",
        "sample_size = 4096*5\n",
        "steps_per_epoch = int(sample_size/batch_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  MSG,KEY = geterateDataset(TEXT_SIZE,KEY_SIZE,sample_size)\n",
        "  running_loss = 0.0\n",
        "  for i in range(steps_per_epoch):\n",
        "    msg = MSG[i]\n",
        "    key = KEY[i]\n",
        "\n",
        "    # ============ Forward ============\n",
        "    \n",
        "    t1 = torch.cat((msg,key)).t().cuda()\n",
        "    alice_output = Alice(t1.cuda())    \n",
        "\n",
        "    t2 = torch.cat((alice_output.t().cuda(),key.cuda()))\n",
        "    bob_output = Bob(t2.cuda())\n",
        "\n",
        "    t3 = alice_output.t().cuda()\n",
        "    eve_output = Eve(t3.cuda())\n",
        "    eve_pred_loss = eveLoss(msg.cuda(),eve_output.cuda())\n",
        "\n",
        "    \n",
        "    bob_pred_loss = aliceBobLoss(msg.cuda(),bob_output,eve_pred_loss)\n",
        "    alice_loss = bob_pred_loss\n",
        "    # print(bob_pred_loss)\n",
        "    # ============ Backward ============\n",
        "    optimizerAlice.zero_grad()\n",
        "    bob_pred_loss.backward(retain_graph=True)\n",
        "    # optimizerAlice.step()\n",
        "\n",
        "    optimizerBob.zero_grad()\n",
        "    alice_loss.backward()\n",
        "    optimizerBob.step()\n",
        "\n",
        "    # ============ Logging ===========\n",
        "    running_loss += bob_pred_loss.data\n",
        "\n",
        "    if epoch%20 ==0 and i==4:\n",
        "      print(msg, alice_output,bob_output, eve_output)\n",
        "\n",
        "    # if i == 4:\n",
        "    #   print('[%d, %5d] loss: %.3f' %\n",
        "\t\t# \t\t\t(epoch + 1, i + 1, running_loss))\n",
        "    #   running_loss = 0.0\n",
        "\n",
        "\n",
        "    #//////////////////////////////////////////////\n",
        "\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i in range(steps_per_epoch):\n",
        "    msg = MSG[i]\n",
        "    key = KEY[i]\n",
        "\n",
        "    # ============ Forward ============\n",
        "    \n",
        "    t1 = torch.cat((msg,key)).t().cuda()\n",
        "    alice_output = Alice(t1.cuda())    \n",
        "\n",
        "    t2 = alice_output.t().cuda()\n",
        "    eve_output = Eve(t2.cuda())\n",
        "    \n",
        "    eve_pred_loss = eveLoss(msg.cuda(),eve_output)\n",
        "\n",
        "    # ============ Backward ============\n",
        "    optimizerEve.zero_grad()\n",
        "    eve_pred_loss.backward(retain_graph=True)\n",
        "    optimizerEve.step()\n",
        "\n",
        "\n",
        "    # ============ Logging ===========\n",
        "    running_loss += eve_pred_loss.data\n",
        "\n",
        "    if epoch%20 ==0 and i==4:\n",
        "      print(msg, alice_output, eve_output)\n",
        "    # if i == 4:\n",
        "    #   print('[%d, %5d] loss: %.3f' %\n",
        "    #       (epoch + 1, i + 1, running_loss / 2000))\n",
        "    #   running_loss = 0.0    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ziyGjCRmeKO",
        "outputId": "50897949-94ab-4b2d-9ba4-501d56eaf43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.]) tensor([0.0262, 0.0260, 0.0259, 0.0261, 0.0259, 0.0259, 0.0258, 0.0261, 0.0261,\n",
            "        0.0261, 0.0259, 0.0260, 0.0261, 0.0261, 0.0260, 0.0259],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.0546, 0.0547, 0.0545, 0.0546, 0.0548, 0.0547, 0.0544, 0.0547, 0.0547,\n",
            "        0.0545, 0.0547, 0.0546, 0.0547, 0.0546, 0.0546, 0.0546],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.1451, -0.1452, -0.1452, -0.1451, -0.1452, -0.1452, -0.1452, -0.1452,\n",
            "        -0.1451, -0.1452, -0.1451, -0.1452, -0.1451, -0.1452, -0.1451, -0.1453],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([ 1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.]) tensor([0.0262, 0.0260, 0.0259, 0.0261, 0.0259, 0.0259, 0.0258, 0.0261, 0.0261,\n",
            "        0.0261, 0.0259, 0.0260, 0.0261, 0.0261, 0.0260, 0.0259],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.1452, -0.1453, -0.1453, -0.1452, -0.1453, -0.1453, -0.1453, -0.1453,\n",
            "        -0.1452, -0.1453, -0.1453, -0.1454, -0.1452, -0.1454, -0.1453, -0.1454],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "         1.,  1.]) tensor([0.0261, 0.0259, 0.0257, 0.0259, 0.0261, 0.0261, 0.0259, 0.0260, 0.0260,\n",
            "        0.0261, 0.0260, 0.0258, 0.0260, 0.0261, 0.0259, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.1160, 0.1159, 0.1160, 0.1159, 0.1160, 0.1161, 0.1160, 0.1160, 0.1160,\n",
            "        0.1159, 0.1159, 0.1161, 0.1161, 0.1159, 0.1161, 0.1160],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0945, -0.0946, -0.0946, -0.0945, -0.0946, -0.0946, -0.0946, -0.0946,\n",
            "        -0.0945, -0.0946, -0.0945, -0.0946, -0.0945, -0.0946, -0.0945, -0.0948],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "         1.,  1.]) tensor([0.0261, 0.0259, 0.0257, 0.0259, 0.0261, 0.0261, 0.0259, 0.0260, 0.0260,\n",
            "        0.0261, 0.0260, 0.0258, 0.0260, 0.0261, 0.0259, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0874, -0.0875, -0.0876, -0.0874, -0.0876, -0.0876, -0.0876, -0.0875,\n",
            "        -0.0874, -0.0875, -0.0875, -0.0876, -0.0874, -0.0876, -0.0875, -0.0877],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1.]) tensor([0.0262, 0.0259, 0.0258, 0.0259, 0.0260, 0.0261, 0.0260, 0.0260, 0.0259,\n",
            "        0.0262, 0.0260, 0.0261, 0.0259, 0.0261, 0.0259, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.1206, 0.1203, 0.1205, 0.1205, 0.1204, 0.1205, 0.1204, 0.1206, 0.1204,\n",
            "        0.1205, 0.1203, 0.1205, 0.1205, 0.1203, 0.1206, 0.1203],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0886, -0.0887, -0.0887, -0.0886, -0.0887, -0.0887, -0.0887, -0.0887,\n",
            "        -0.0886, -0.0887, -0.0887, -0.0888, -0.0886, -0.0888, -0.0887, -0.0889],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1.]) tensor([0.0262, 0.0259, 0.0258, 0.0259, 0.0260, 0.0261, 0.0260, 0.0260, 0.0259,\n",
            "        0.0262, 0.0260, 0.0261, 0.0259, 0.0261, 0.0259, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0834, -0.0835, -0.0836, -0.0834, -0.0836, -0.0835, -0.0836, -0.0835,\n",
            "        -0.0834, -0.0835, -0.0835, -0.0836, -0.0834, -0.0836, -0.0835, -0.0837],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([ 1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.]) tensor([0.0258, 0.0259, 0.0259, 0.0262, 0.0263, 0.0259, 0.0258, 0.0260, 0.0261,\n",
            "        0.0261, 0.0260, 0.0260, 0.0261, 0.0258, 0.0260, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.2018, 0.2018, 0.2017, 0.2018, 0.2016, 0.2018, 0.2019, 0.2018, 0.2016,\n",
            "        0.2019, 0.2017, 0.2017, 0.2017, 0.2017, 0.2017, 0.2017],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0071, -0.0071, -0.0072, -0.0071, -0.0072, -0.0072, -0.0072, -0.0072,\n",
            "        -0.0071, -0.0071, -0.0071, -0.0072, -0.0071, -0.0072, -0.0071, -0.0075],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([ 1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.]) tensor([0.0258, 0.0259, 0.0259, 0.0262, 0.0263, 0.0259, 0.0258, 0.0260, 0.0261,\n",
            "        0.0261, 0.0260, 0.0260, 0.0261, 0.0258, 0.0260, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([-0.0043, -0.0044, -0.0045, -0.0044, -0.0045, -0.0045, -0.0045, -0.0044,\n",
            "        -0.0044, -0.0044, -0.0044, -0.0045, -0.0043, -0.0045, -0.0044, -0.0048],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1., -1.]) tensor([0.0260, 0.0260, 0.0258, 0.0260, 0.0261, 0.0263, 0.0259, 0.0258, 0.0262,\n",
            "        0.0260, 0.0260, 0.0262, 0.0260, 0.0260, 0.0260, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.2198, 0.2201, 0.2198, 0.2199, 0.2199, 0.2199, 0.2200, 0.2198, 0.2198,\n",
            "        0.2199, 0.2199, 0.2199, 0.2198, 0.2198, 0.2199, 0.2199],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.0151, 0.0150, 0.0150, 0.0151, 0.0150, 0.0150, 0.0150, 0.0150, 0.0151,\n",
            "        0.0151, 0.0150, 0.0149, 0.0151, 0.0149, 0.0150, 0.0147],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor([-1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1., -1.]) tensor([0.0260, 0.0260, 0.0258, 0.0260, 0.0261, 0.0263, 0.0259, 0.0258, 0.0262,\n",
            "        0.0260, 0.0260, 0.0262, 0.0260, 0.0260, 0.0260, 0.0260],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0.0138, 0.0137, 0.0137, 0.0138, 0.0136, 0.0137, 0.0136, 0.0137, 0.0138,\n",
            "        0.0137, 0.0137, 0.0136, 0.0138, 0.0136, 0.0137, 0.0133],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qqKpYCgYapcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.L1Loss()\n",
        "input = torch.randn(1,2, requires_grad=True)\n",
        "target = torch.randn(1, 2)\n",
        "output = loss(input, target)\n",
        "print((output))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm0jNA5ZUVqe",
        "outputId": "5ab71f60-2453-4ddf-a6ca-f7ae20081a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.3193, grad_fn=<L1LossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "haha = torch.randint(0,2,(3,))\n",
        "haha = torch.cat((haha,haha))\n",
        "haha = haha.float()\n",
        "# input = F.pad(input, (1,2))\n",
        "# input = input + torch.randn(3)\n",
        "\n",
        "# input = torch.nn.ZeroPad2d((3,0))"
      ],
      "metadata": {
        "id": "GM-SAyTIBzE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(haha,type(haha))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_aE5ACgCAZB",
        "outputId": "0548d61e-98a8-44e6-e3de-e2552c15e8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 1., 1., 0., 1.]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    }
  ]
}